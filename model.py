# -*- coding: utf-8 -*-
"""NLP_Project_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hc8n7vr7SyvBOp8A9LIz1iGn7EY4xO6Y
"""

import string
import json
import os
import pandas as pd
from pandas import json_normalize
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from sklearn import svm
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from scipy.sparse import hstack
pd.set_option('display.max_columns', 10)

from google.colab import files
uploaded = files.upload()

# Load Data
def readJSON(path):
    with open(path) as f:
        return json_normalize(json.load(f))

def readCSV(path):
  return pd.read_csv(path, index_col=0)

rawData = readCSV('./train.csv')

nltk.download('stopwords')
nltk.download('wordnet')

# Preprocessing

def preprocess(raw):
  def remove_punc(text):
    cleaned_text = ''.join([c for c in text if c not in string.punctuation])
    return cleaned_text
  
  def remove_stopwords(text):
    words = [w for w in text if w not in stopwords.words('english')]
    return words
  
  def lemma(text):
    lemmatizer = WordNetLemmatizer()
    lem_text = [lemmatizer.lemmatize(i) for i in text]
    return lem_text

  def tokenize(text):
    tokenizer = RegexpTokenizer(r'\w+')
    return tokenizer.tokenize(text.lower())

  def processText(text):
    text = remove_punc(text)
    text = tokenize(text)
    text = remove_stopwords(text)
    text = lemma(text)
    return text

  # Copy
  data = raw.copy()

  # Drop Field
  data.drop(columns=['context_idx', 'mp4'], inplace=True)

  # Deal with missing value
  data['reply'] = data['reply'].apply(lambda x: '' if pd.isna(x) else x)
  
  # Deal with text
  data['text'] = data['text'].apply(processText)

  return data

data = preprocess(rawData)

# Prepare for training

# # Shuffle and split
# shuffledData = data.sample(frac=1)
# divider = shuffledData.shape[0] // 3
# testDF = shuffledData[:divider].copy()
# trainDF = shuffledData[divider:].copy()

# Split real and fake
realDF = data[data.label == 'real']
fakeDF = data[data.label == 'fake']

# Construct train set and test set
trainSize = 10000
trainDF = pd.concat([realDF[:trainSize], fakeDF[:trainSize]])
testDF = pd.concat([realDF[trainSize:], fakeDF[trainSize:]])

# Encode label
le = LabelEncoder()
label = le.fit_transform(data['label'])
trainLabel = le.transform(trainDF['label'])
testLabel = le.transform(testDF['label'])

# Remove label
trainDF.drop(columns=['label'], inplace=True)
testDF.drop(columns=['label'], inplace=True)

real = np.count_nonzero(trainLabel == list(le.classes_).index('real'))
fake = np.count_nonzero(trainLabel == list(le.classes_).index('fake'))
print(f'Train: {{shape: {trainDF.shape}, real: {real}, fake: {fake}}}')

real = np.count_nonzero(testLabel == list(le.classes_).index('real'))
fake = np.count_nonzero(testLabel == list(le.classes_).index('fake'))
print(f'Test: {{shape: {testDF.shape}, real: {real}, fake: {fake}}}')

# TF-IDF
textVectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=lambda i:i, max_df=0.9, min_df=0.05, lowercase=False)
replyVectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=lambda i:i, max_df=0.9, min_df=0.05, lowercase=False)

trainTextTfidf = textVectorizer.fit_transform(trainDF["text"])
testTextTfidf = textVectorizer.transform(testDF["text"])

trainReplyTfidf = replyVectorizer.fit_transform(trainDF["reply"])
testReplyTfidf = replyVectorizer.transform(testDF["reply"])

train = hstack((trainTextTfidf, trainReplyTfidf))
test = hstack((testTextTfidf, testReplyTfidf))

# vectorizer.fit(pd.concat([realDF[:trainSize], fakeDF[:trainSize]]))
# train = hstack((vectorizer.transform(trainDF['text']), vectorizer.transform(trainDF['reply'])))
# test = hstack((vectorizer.transform(testDF['text']), vectorizer.transform(testDF['reply'])))

print(train.shape)
print(test.shape)

# Train Models

def trainModel(name, model, trainX, testX, trainT, testT):

  def printScore(name, T, Y):
    print(f'[{name}]\n'
        f'Accuracy: {accuracy_score(T, Y):0.5f}, '
        f'Precision: {precision_score(T, Y):0.5f}, '
        f'Recall: {recall_score(T, Y):0.5f}, '
        f'f1: {f1_score(T, Y):0.5f}, '
        f'ROC: {roc_auc_score(T, Y):0.5f}\n' 
    )   

  # Train
  model.fit(trainX, trainT)

  # Predict
  trainY = model.predict(trainX)
  testY = model.predict(testX)

  # Evaluate
  printScore(f'{name} Train', trainT, trainY)
  printScore(f'{name} Test', testT, testY)

models = [
  dict(name = 'LinearSVC', model = svm.LinearSVC()),
  dict(name = 'LogisticRegression', model = LogisticRegression()),
  dict(name = 'SGD', model = SGDClassifier()),
  dict(name = 'NaiveBayes', model = MultinomialNB()),
  dict(name = 'GradientBoost', model = GradientBoostingClassifier())
]

for m in models:
    trainModel(m['name'], m['model'], train, test, trainLabel, testLabel)

# Load and Preprocess New Data
newRawData = readJSON('/Users/alan/Downloads/pubilc_data/dev.json')
newData = preprocess(newRawData)

# TF-IDF
newTextTfidf = textVectorizer.transform(newData["text"])
newReplyTfidf = replyVectorizer.transform(newData["reply"])

newX = hstack((newTextTfidf, newReplyTfidf))

# Output
os.makedirs('output', exist_ok=True)
output = newRawData[['idx', 'context_idx']].copy()
def predict(name, model, X):
    Y = model.predict(X)
    output['label'] = le.inverse_transform(Y)
    output.to_csv(f'./output/{name}.csv', index=False)
for m in models:
    predict(m['name'], m['model'], newX)

